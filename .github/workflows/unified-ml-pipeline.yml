# Unified ML CI/CD Pipeline
# Single pipeline: Local prep â†’ Azure ML training â†’ Regression test â†’ Version
# Combines the best of local (fast prep/validation) and cloud (scalable training)

name: Unified ML Pipeline

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.9'
  RESOURCE_GROUP: 'cw2-mlops-rg'
  WORKSPACE_NAME: 'cw2-mlops-workspace'
  COMPUTE_NAME: 'cpu-cluster-fast'

jobs:
  # Job 1: Data Preprocessing (Local - Fast)
  preprocess-data:
    name: Data Preprocessing
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas numpy scikit-learn joblib

    - name: Run preprocessing
      run: |
        python preprocess.py

    - name: Upload processed data
      uses: actions/upload-artifact@v4
      with:
        name: processed-data
        path: processed_data/
        retention-days: 7

  # Job 2: Submit Training to Azure ML (Cloud Training)
  train-on-azure:
    name: Train Models on Azure ML
    runs-on: ubuntu-latest
    needs: preprocess-data

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Azure ML SDK
      run: |
        python -m pip install --upgrade pip
        pip install azure-ai-ml azure-identity azureml-mlflow

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Submit Training Job to Azure ML
      id: submit_job
      run: |
        python << 'EOF'
        from azure.ai.ml import MLClient, command, Input, Output
        from azure.ai.ml.entities import Environment
        from azure.identity import DefaultAzureCredential
        import os

        print("="*70)
        print("SUBMITTING TRAINING JOB TO AZURE ML")
        print("="*70)

        # Connect to workspace
        ml_client = MLClient(
            DefaultAzureCredential(),
            subscription_id="${{ secrets.AZURE_SUBSCRIPTION_ID }}",
            resource_group_name="${{ env.RESOURCE_GROUP }}",
            workspace_name="${{ env.WORKSPACE_NAME }}"
        )

        print(f"\nâœ… Connected to workspace: ${{ env.WORKSPACE_NAME }}")

        # Get dataset
        data_asset = ml_client.data.get(name="support-tickets-dataset", version="1")
        print(f"âœ… Dataset found: {data_asset.name}")

        # Create environment
        env = Environment(
            name="mlops-training-env",
            conda_file="environment.yml",
            image="mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest"
        )

        # Create training job
        # Azure ML will download the dataset and pass its path to the script
        # IMPORTANT: Quote the path to handle spaces in filename
        train_command = 'python train_azure.py --data_path "{inputs.dataset_input}"'

        job = command(
            code="./",
            command=train_command,
            inputs={
                "dataset_input": Input(
                    type="uri_file",
                    path=data_asset.id
                )
            },
            outputs={
                "outputs": Output(type="uri_folder")
            },
            environment=env,
            compute="${{ env.COMPUTE_NAME }}",
            experiment_name="unified-cicd-training",
            display_name="unified-run-${{ github.run_number }}"
        )

        # Submit job
        returned_job = ml_client.jobs.create_or_update(job)
        print(f"\nâœ… Job submitted successfully!")
        print(f"   Job Name: {returned_job.name}")
        print(f"   Status: {returned_job.status}")
        print(f"   Studio URL: https://ml.azure.com/runs/{returned_job.name}")

        # Save job info for next step
        with open("job_name.txt", "w") as f:
            f.write(returned_job.name)

        # Wait for completion
        print("\nâ³ Waiting for training to complete...")
        print("   (This may take 15-20 minutes on first run)")
        ml_client.jobs.stream(returned_job.name)

        # Get final status
        final_job = ml_client.jobs.get(returned_job.name)
        print(f"\nðŸ“Š Training completed!")
        print(f"   Final Status: {final_job.status}")

        if final_job.status != "Completed":
            print(f"\nâŒ Training failed with status: {final_job.status}")
            exit(1)

        print("\nâœ… Azure ML training completed successfully!")
        EOF

    - name: Download Models and Metrics from Azure ML
      run: |
        python << 'EOF'
        from azure.ai.ml import MLClient
        from azure.identity import DefaultAzureCredential
        import json
        import os

        print("\n" + "="*70)
        print("DOWNLOADING MODELS AND METRICS FROM AZURE ML")
        print("="*70)

        # Connect to workspace
        ml_client = MLClient(
            DefaultAzureCredential(),
            subscription_id="${{ secrets.AZURE_SUBSCRIPTION_ID }}",
            resource_group_name="${{ env.RESOURCE_GROUP }}",
            workspace_name="${{ env.WORKSPACE_NAME }}"
        )

        # Read job name
        with open("job_name.txt", "r") as f:
            job_name = f.read().strip()

        print(f"\nDownloading outputs from job: {job_name}")

        # Get the job
        job = ml_client.jobs.get(job_name)
        print(f"âœ… Job status: {job.status}")

        # Download job outputs (this includes the models and metrics)
        print("\nðŸ“¥ Downloading job outputs...")
        ml_client.jobs.download(name=job_name, download_path=".", output_name="outputs")
        print("âœ… Outputs downloaded")

        # The outputs are downloaded to ./artifacts/{job_name}/outputs/
        # We need to reorganize them into our expected structure
        import shutil

        artifacts_path = f"./artifacts/{job_name}/outputs"

        # Create models directory structure
        os.makedirs("models/iteration_1", exist_ok=True)
        os.makedirs("models/iteration_2", exist_ok=True)

        # Copy model files
        if os.path.exists(f"{artifacts_path}/iteration_1_model.pkl"):
            shutil.copy(f"{artifacts_path}/iteration_1_model.pkl", "models/iteration_1/iteration_1_model.pkl")
            print("âœ… Copied iteration_1_model.pkl")

        if os.path.exists(f"{artifacts_path}/iteration_2_model.pkl"):
            shutil.copy(f"{artifacts_path}/iteration_2_model.pkl", "models/iteration_2/iteration_2_model.pkl")
            print("âœ… Copied iteration_2_model.pkl")

        # Copy metrics files if they exist
        if os.path.exists(f"{artifacts_path}/iteration_1_metrics.json"):
            shutil.copy(f"{artifacts_path}/iteration_1_metrics.json", "models/iteration_1/metrics.json")
            print("âœ… Copied iteration_1 metrics")

        if os.path.exists(f"{artifacts_path}/iteration_2_metrics.json"):
            shutil.copy(f"{artifacts_path}/iteration_2_metrics.json", "models/iteration_2/metrics.json")
            print("âœ… Copied iteration_2 metrics")

        print("\nâœ… Models and metrics downloaded and organized")
        print(f"   View job at: https://ml.azure.com/runs/{job_name}")
        EOF

    - name: Save Model Information
      run: |
        echo "Models trained successfully and ready for deployment"
        echo "Skipping Model Registry - using downloaded model files directly"

        # Just save a marker that models are ready
        echo "support-ticket-classifier" > model_name.txt
        echo "${{ github.run_number }}" > model_version.txt

    - name: Upload models and metrics
      uses: actions/upload-artifact@v4
      with:
        name: azure-trained-models
        path: models/
        retention-days: 30

    - name: Create training summary
      run: |
        echo "### Azure ML Training Results :rocket:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Training completed on Azure ML compute cluster**" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [ -f job_name.txt ]; then
          JOB_NAME=$(cat job_name.txt)
          echo "- **Job Name**: \`$JOB_NAME\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Workspace**: ${{ env.WORKSPACE_NAME }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Compute**: ${{ env.COMPUTE_NAME }}" >> $GITHUB_STEP_SUMMARY
          echo "- **View in Azure**: [Open in Studio](https://ml.azure.com/runs/$JOB_NAME)" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "âœ… Iteration 1: Random Forest (Baseline)" >> $GITHUB_STEP_SUMMARY
        echo "âœ… Iteration 2: XGBoost (Improved)" >> $GITHUB_STEP_SUMMARY

  # Job 3: Regression Testing (Local - Fast Validation)
  regression-test:
    name: Regression Testing
    runs-on: ubuntu-latest
    needs: train-on-azure

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Download models and metrics
      uses: actions/download-artifact@v4
      with:
        name: azure-trained-models
        path: models/

    - name: Run regression testing
      id: regression_test
      run: |
        echo "="*70
        echo "REGRESSION TESTING"
        echo "="*70
        echo ""
        echo "Comparing Iteration 2 (XGBoost) vs Iteration 1 (Random Forest)"
        echo "Threshold: 2% performance drop allowed"
        echo ""
        python evaluate.py
      continue-on-error: false  # Pipeline MUST fail if regression detected

    - name: Upload evaluation results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: evaluation-results
        path: evaluation_results/

    - name: Display regression test results
      if: always()
      run: |
        echo "### Regression Testing Results :bar_chart:" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [ -f evaluation_results/evaluation_report.json ]; then
          cat evaluation_results/evaluation_report.json
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
          cat evaluation_results/evaluation_report.json >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        fi

  # Job 4: Version Models (Only if regression test passed)
  version-models:
    name: Version Models
    runs-on: ubuntu-latest
    needs: regression-test

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Download models
      uses: actions/download-artifact@v4
      with:
        name: azure-trained-models
        path: models/

    - name: Download evaluation results
      uses: actions/download-artifact@v4
      with:
        name: evaluation-results
        path: evaluation_results/

    - name: Create model version tag
      run: |
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        echo "MODEL_VERSION=v_${TIMESTAMP}" >> $GITHUB_ENV
        echo "âœ… Created version tag: v_${TIMESTAMP}"

    - name: Upload versioned models
      uses: actions/upload-artifact@v4
      with:
        name: versioned-models
        path: models/
        retention-days: 30

  # Job 5: Deploy to Azure ML Online Endpoint
  deploy-endpoint:
    name: Deploy to Online Endpoint
    runs-on: ubuntu-latest
    needs: version-models
    outputs:
      scoring_uri: ${{ steps.get_endpoint.outputs.scoring_uri }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Azure ML SDK
      run: |
        python -m pip install --upgrade pip
        pip install azure-ai-ml azure-identity

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Download versioned models
      uses: actions/download-artifact@v4
      with:
        name: versioned-models
        path: models/

    - name: Create scoring script
      run: |
        cat > score.py << 'EOF'
        import json
        import pickle
        import numpy as np
        from sklearn.preprocessing import StandardScaler

        def init():
            global model, scaler
            # Load the XGBoost model (iteration 2)
            with open("iteration_2_model.pkl", "rb") as f:
                model = pickle.load(f)
            print("Model loaded successfully")

        def run(raw_data):
            try:
                data = json.loads(raw_data)
                input_data = np.array(data["data"])

                # Make prediction
                predictions = model.predict(input_data)

                # Remap back from 0,1,2 to 1,2,3
                predictions = predictions + 1

                return json.dumps({"predictions": predictions.tolist()})
            except Exception as e:
                return json.dumps({"error": str(e)})
        EOF

    - name: Deploy to Azure ML Endpoint
      id: deploy_endpoint
      run: |
        python << 'EOF'
        from azure.ai.ml import MLClient
        from azure.ai.ml.entities import (
            ManagedOnlineEndpoint,
            ManagedOnlineDeployment,
            Environment,
            CodeConfiguration
        )
        from azure.identity import DefaultAzureCredential
        import os

        print("="*70)
        print("DEPLOYING TO AZURE ML ONLINE ENDPOINT")
        print("="*70)

        # Connect to workspace
        ml_client = MLClient(
            DefaultAzureCredential(),
            subscription_id="${{ secrets.AZURE_SUBSCRIPTION_ID }}",
            resource_group_name="${{ env.RESOURCE_GROUP }}",
            workspace_name="${{ env.WORKSPACE_NAME }}"
        )

        endpoint_name = "support-ticket-classifier"

        # Create or update endpoint
        endpoint = ManagedOnlineEndpoint(
            name=endpoint_name,
            description="Support ticket priority classifier",
            auth_mode="key"
        )

        print(f"\nðŸ“¦ Creating/updating endpoint: {endpoint_name}")
        ml_client.online_endpoints.begin_create_or_update(endpoint).result()
        print("âœ… Endpoint ready")

        # Deploy directly from local model files (no Model Registry)
        # Copy model file to deployment directory
        import shutil

        # Debug: List available files
        print("\nðŸ” Checking models directory:")
        for root, dirs, files in os.walk("./models"):
            for f in files:
                print(f"  Found: {os.path.join(root, f)}")

        os.makedirs("deployment", exist_ok=True)
        shutil.copy("./models/iteration_2/iteration_2_model.pkl", "./deployment/iteration_2_model.pkl")
        shutil.copy("score.py", "./deployment/score.py")

        print("\nðŸ“¦ Preparing deployment package (without Model Registry)...")

        # Create deployment directly from local files
        deployment = ManagedOnlineDeployment(
            name="blue",
            endpoint_name=endpoint_name,
            code_configuration=CodeConfiguration(
                code="./deployment",
                scoring_script="score.py"
            ),
            environment=Environment(
                conda_file="environment.yml",
                image="mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest"
            ),
            instance_type="Standard_DS2_v2",
            instance_count=1
        )

        print(f"\nðŸš€ Deploying model to endpoint...")
        print("   (Deploying directly from files, skipping Model Registry)")
        ml_client.online_deployments.begin_create_or_update(deployment).result()
        print("âœ… Deployment complete")

        # Route 100% traffic to blue deployment
        endpoint.traffic = {"blue": 100}
        ml_client.online_endpoints.begin_create_or_update(endpoint).result()
        print("âœ… Traffic routed to deployment")

        # Get endpoint details
        endpoint = ml_client.online_endpoints.get(endpoint_name)
        print(f"\nðŸ“Š Endpoint URL: {endpoint.scoring_uri}")

        # Save for next job
        with open("endpoint_uri.txt", "w") as f:
            f.write(endpoint.scoring_uri)

        print("\nâœ… Deployment successful (without Model Registry)!")
        EOF

    - name: Get endpoint URI
      id: get_endpoint
      run: |
        SCORING_URI=$(cat endpoint_uri.txt)
        echo "scoring_uri=$SCORING_URI" >> $GITHUB_OUTPUT
        echo "âœ… Scoring URI: $SCORING_URI"

    - name: Upload endpoint info
      uses: actions/upload-artifact@v4
      with:
        name: endpoint-info
        path: endpoint_uri.txt

  # Job 6: Load Testing with Locust
  load-testing:
    name: Load Testing with Locust
    runs-on: ubuntu-latest
    needs: deploy-endpoint

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Locust
      run: |
        python -m pip install --upgrade pip
        pip install locust requests azure-identity azure-ai-ml

    - name: Download endpoint info
      uses: actions/download-artifact@v4
      with:
        name: endpoint-info

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Create Locust test file with endpoint
      run: |
        cat > locustfile_endpoint.py << 'EOF'
        from locust import HttpUser, task, between
        import json
        import random
        import os

        # Get endpoint URI from environment
        SCORING_URI = os.getenv("SCORING_URI", "")
        API_KEY = os.getenv("API_KEY", "")

        class SupportTicketPredictionUser(HttpUser):
            wait_time = between(1, 3)
            host = SCORING_URI.replace("/score", "") if SCORING_URI else "http://localhost:8000"

            sample_tickets = [
                {
                    "org_users": 150,
                    "past_30d_tickets": 12,
                    "customers_affected": 5,
                    "error_rate_pct": 2.5,
                    "downtime_min": 30,
                    "description_length": 120,
                    "resolution_time_hours": 4,
                    "customer_satisfaction_score": 7.5,
                    "revenue_dollars": 50000,
                    "api_calls_per_day": 10000,
                    "team_size": 8,
                    "satisfaction_score_0_10": 7,
                    "day_of_week_num": 3,
                    "company_size_cat": 2,
                    "industry_cat": 1,
                    "customer_tier_cat": 2,
                    "region_cat": 1,
                    "product_area_cat": 3,
                    "booking_channel_cat": 1,
                    "reported_by_role_cat": 2,
                    "customer_sentiment_cat": 1,
                    "payment_impact_flag": 0,
                    "data_loss_flag": 0,
                    "has_runbook": 1
                }
            ]

            @task
            def predict_priority(self):
                ticket_data = random.choice(self.sample_tickets)
                headers = {
                    'Content-Type': 'application/json',
                    'Authorization': f'Bearer {API_KEY}'
                }
                payload = {"data": [list(ticket_data.values())]}

                try:
                    self.client.post("/score", json=payload, headers=headers)
                except Exception as e:
                    print(f"Request failed: {e}")
        EOF

    - name: Get API Key
      id: get_key
      run: |
        python << 'EOF'
        from azure.ai.ml import MLClient
        from azure.identity import DefaultAzureCredential

        ml_client = MLClient(
            DefaultAzureCredential(),
            subscription_id="${{ secrets.AZURE_SUBSCRIPTION_ID }}",
            resource_group_name="${{ env.RESOURCE_GROUP }}",
            workspace_name="${{ env.WORKSPACE_NAME }}"
        )

        keys = ml_client.online_endpoints.get_keys("support-ticket-classifier")
        api_key = keys.primary_key

        with open("api_key.txt", "w") as f:
            f.write(api_key)

        print("âœ… API key retrieved")
        EOF

    - name: Run Locust Load Test
      run: |
        SCORING_URI=$(cat endpoint_uri.txt)
        API_KEY=$(cat api_key.txt)

        echo "="*70
        echo "RUNNING LOAD TEST"
        echo "="*70
        echo "Endpoint: $SCORING_URI"
        echo "Users: 50"
        echo "Spawn rate: 5/sec"
        echo "Duration: 60 seconds"

        export SCORING_URI=$SCORING_URI
        export API_KEY=$API_KEY

        locust -f locustfile_endpoint.py \
          --headless \
          --users 50 \
          --spawn-rate 5 \
          --run-time 60s \
          --html load_test_report.html \
          --csv load_test_results

    - name: Upload load test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: load-test-results
        path: |
          load_test_report.html
          load_test_results*.csv

    - name: Display load test summary
      if: always()
      run: |
        echo "### ðŸ”¥ Load Testing Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Test Configuration:**" >> $GITHUB_STEP_SUMMARY
        echo "- Users: 50" >> $GITHUB_STEP_SUMMARY
        echo "- Spawn rate: 5/sec" >> $GITHUB_STEP_SUMMARY
        echo "- Duration: 60 seconds" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f load_test_results_stats.csv ]; then
          echo "**Results:**" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          head -20 load_test_results_stats.csv >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        fi

  # Job 7: Pipeline Summary
  pipeline-summary:
    name: Pipeline Summary
    runs-on: ubuntu-latest
    needs: [version-models, deploy-endpoint, load-testing]
    if: always()

    steps:
    - name: Download evaluation results
      uses: actions/download-artifact@v4
      with:
        name: evaluation-results
        path: evaluation_results/

    - name: Generate final summary
      run: |
        echo "### ðŸŽ‰ Unified ML Pipeline - COMPLETE!" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "#### Pipeline Flow" >> $GITHUB_STEP_SUMMARY
        echo "1. âœ… Data preprocessing (local)" >> $GITHUB_STEP_SUMMARY
        echo "2. âœ… Model training (Azure ML cloud)" >> $GITHUB_STEP_SUMMARY
        echo "3. âœ… Regression testing (local)" >> $GITHUB_STEP_SUMMARY
        echo "4. âœ… Model versioning" >> $GITHUB_STEP_SUMMARY
        echo "5. âœ… Endpoint deployment (Azure ML)" >> $GITHUB_STEP_SUMMARY
        echo "6. âœ… Load testing (Locust)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "#### Execution Details" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Run**: #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "#### Model Performance" >> $GITHUB_STEP_SUMMARY
        if [ -f evaluation_results/evaluation_report.json ]; then
          echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
          cat evaluation_results/evaluation_report.json >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "#### Deployment" >> $GITHUB_STEP_SUMMARY
        echo "- **Endpoint**: support-ticket-classifier" >> $GITHUB_STEP_SUMMARY
        echo "- **Status**: Live and tested" >> $GITHUB_STEP_SUMMARY
        echo "- **Load test**: Passed (50 users, 60s)" >> $GITHUB_STEP_SUMMARY

    - name: Success notification
      run: |
        echo "========================================================"
        echo "ðŸŽ‰ COMPLETE PIPELINE SUCCESSFUL!"
        echo "========================================================"
        echo "âœ… Data preprocessed locally"
        echo "âœ… Models trained on Azure ML cloud"
        echo "âœ… Regression testing passed"
        echo "âœ… Models versioned and deployed"
        echo "âœ… Endpoint live and load tested"
        echo ""
        echo "All stages completed successfully!"
        echo "========================================================"
